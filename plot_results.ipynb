{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Results from All Algorithm Runs\n",
        "\n",
        "This notebook loads and plots results from all three algorithms (sgd_relu, alt_em_sgd, alt_em_closed_form) that were run sequentially on the same data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Set the output directory path\n",
        "# You can either specify a specific experiment directory or use the most recent one\n",
        "outputs_dir = \"outputs\"\n",
        "\n",
        "# Option 1: Use the most recent experiment\n",
        "experiment_dirs = sorted(glob.glob(os.path.join(outputs_dir, \"*\")))\n",
        "if experiment_dirs:\n",
        "    experiment_dir = experiment_dirs[-1]  # Most recent\n",
        "    print(f\"Using experiment directory: {experiment_dir}\")\n",
        "else:\n",
        "    raise ValueError(f\"No experiment directories found in {outputs_dir}\")\n",
        "\n",
        "# Option 2: Or specify a specific directory\n",
        "# experiment_dir = \"outputs/your_experiment_name_20250101_120000\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data for all algorithms\n",
        "algorithms = [\"sgd_relu\", \"alt_em_sgd\", \"alt_em_closed_form\"]\n",
        "data = {}\n",
        "\n",
        "for algo in algorithms:\n",
        "    algo_dir = os.path.join(experiment_dir, algo)\n",
        "    if not os.path.exists(algo_dir):\n",
        "        print(f\"Warning: Directory {algo_dir} not found, skipping {algo}\")\n",
        "        continue\n",
        "    \n",
        "    # Load training history\n",
        "    history_path = os.path.join(algo_dir, \"training_history.csv\")\n",
        "    if os.path.exists(history_path):\n",
        "        data[algo] = {\"history\": pd.read_csv(history_path)}\n",
        "    else:\n",
        "        print(f\"Warning: {history_path} not found\")\n",
        "        continue\n",
        "    \n",
        "    # Load final metrics\n",
        "    metrics_path = os.path.join(algo_dir, \"final_metrics.json\")\n",
        "    if os.path.exists(metrics_path):\n",
        "        with open(metrics_path, 'r') as f:\n",
        "            data[algo][\"final_metrics\"] = json.load(f)\n",
        "    \n",
        "    # Load losses\n",
        "    losses_path = os.path.join(algo_dir, \"losses.json\")\n",
        "    if os.path.exists(losses_path):\n",
        "        with open(losses_path, 'r') as f:\n",
        "            data[algo][\"losses\"] = json.load(f)\n",
        "\n",
        "print(f\"Loaded data for {len(data)} algorithms: {list(data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Training and Validation Losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot training losses\n",
        "ax1 = axes[0]\n",
        "for algo, algo_data in data.items():\n",
        "    history = algo_data[\"history\"]\n",
        "    if \"train_loss\" in history.columns:\n",
        "        x_key = \"epoch\" if \"epoch\" in history.columns else \"cycle\"\n",
        "        ax1.plot(history[x_key], history[\"train_loss\"], label=algo, marker='o', markersize=3)\n",
        "ax1.set_xlabel(x_key.capitalize())\n",
        "ax1.set_ylabel(\"Train Loss\")\n",
        "ax1.set_title(\"Training Loss Over Time\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot validation losses\n",
        "ax2 = axes[1]\n",
        "for algo, algo_data in data.items():\n",
        "    history = algo_data[\"history\"]\n",
        "    if \"val_loss\" in history.columns:\n",
        "        x_key = \"epoch\" if \"epoch\" in history.columns else \"cycle\"\n",
        "        ax2.plot(history[x_key], history[\"val_loss\"], label=algo, marker='s', markersize=3)\n",
        "ax2.set_xlabel(x_key.capitalize())\n",
        "ax2.set_ylabel(\"Validation Loss\")\n",
        "ax2.set_title(\"Validation Loss Over Time\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Training and Test Accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot training accuracies\n",
        "ax1 = axes[0]\n",
        "for algo, algo_data in data.items():\n",
        "    history = algo_data[\"history\"]\n",
        "    if \"train_acc\" in history.columns:\n",
        "        x_key = \"epoch\" if \"epoch\" in history.columns else \"cycle\"\n",
        "        ax1.plot(history[x_key], history[\"train_acc\"], label=algo, marker='o', markersize=3)\n",
        "ax1.set_xlabel(x_key.capitalize())\n",
        "ax1.set_ylabel(\"Train Accuracy\")\n",
        "ax1.set_title(\"Training Accuracy Over Time\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot validation accuracies\n",
        "ax2 = axes[1]\n",
        "for algo, algo_data in data.items():\n",
        "    history = algo_data[\"history\"]\n",
        "    if \"val_acc\" in history.columns:\n",
        "        x_key = \"epoch\" if \"epoch\" in history.columns else \"cycle\"\n",
        "        ax2.plot(history[x_key], history[\"val_acc\"], label=algo, marker='s', markersize=3)\n",
        "ax2.set_xlabel(x_key.capitalize())\n",
        "ax2.set_ylabel(\"Validation Accuracy\")\n",
        "ax2.set_title(\"Validation Accuracy Over Time\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Final Metrics Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract final metrics\n",
        "final_train_losses = []\n",
        "final_test_losses = []\n",
        "final_train_accs = []\n",
        "final_test_accs = []\n",
        "algo_names = []\n",
        "\n",
        "for algo, algo_data in data.items():\n",
        "    if \"final_metrics\" in algo_data:\n",
        "        metrics = algo_data[\"final_metrics\"]\n",
        "        final_train_losses.append(metrics.get(\"train_loss\", 0))\n",
        "        final_test_losses.append(metrics.get(\"test_loss\", 0))\n",
        "        final_train_accs.append(metrics.get(\"train_acc\", 0))\n",
        "        final_test_accs.append(metrics.get(\"test_acc\", 0))\n",
        "        algo_names.append(algo)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Final train loss\n",
        "axes[0, 0].bar(algo_names, final_train_losses, color='skyblue')\n",
        "axes[0, 0].set_ylabel(\"Final Train Loss\")\n",
        "axes[0, 0].set_title(\"Final Training Loss Comparison\")\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Final test loss\n",
        "axes[0, 1].bar(algo_names, final_test_losses, color='lightcoral')\n",
        "axes[0, 1].set_ylabel(\"Final Test Loss\")\n",
        "axes[0, 1].set_title(\"Final Test Loss Comparison\")\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Final train accuracy\n",
        "axes[1, 0].bar(algo_names, final_train_accs, color='lightgreen')\n",
        "axes[1, 0].set_ylabel(\"Final Train Accuracy\")\n",
        "axes[1, 0].set_title(\"Final Training Accuracy Comparison\")\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Final test accuracy\n",
        "axes[1, 1].bar(algo_names, final_test_accs, color='plum')\n",
        "axes[1, 1].set_ylabel(\"Final Test Accuracy\")\n",
        "axes[1, 1].set_title(\"Final Test Accuracy Comparison\")\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary DataFrame\n",
        "summary_data = []\n",
        "for algo, algo_data in data.items():\n",
        "    if \"final_metrics\" in algo_data:\n",
        "        metrics = algo_data[\"final_metrics\"]\n",
        "        summary_data.append({\n",
        "            \"Algorithm\": algo,\n",
        "            \"Train Loss\": f\"{metrics.get('train_loss', 0):.6f}\",\n",
        "            \"Test Loss\": f\"{metrics.get('test_loss', 0):.6f}\",\n",
        "            \"Train Acc\": f\"{metrics.get('train_acc', 0):.4f}\",\n",
        "            \"Test Acc\": f\"{metrics.get('test_acc', 0):.4f}\",\n",
        "            \"Val Loss\": f\"{metrics.get('val_loss', 0):.6f}\",\n",
        "            \"Val Acc\": f\"{metrics.get('val_acc', 0):.4f}\"\n",
        "        })\n",
        "\n",
        "if summary_data:\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\nSummary of Final Metrics:\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No summary data available.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
